Innovation einer neuronalen Netzarchitektur, die besser für modellfreie die besser für modellfreies RL geeignet ist
Dieser Ansatz hat den Vorteil, dass das neue Netz leicht mit bestehenden und zukünftigen Algorithmen für RL kombiniert werden kann
Entwickelt ein neues Netzwerk (Abbildung 1), verwendet aber bereits veröffentlichte Algorithmen.
Trennt explizit die Darstellung von Zustandswerten und (zustandsabhängigen) Handlungsvorteilen



Die vorgeschlagene Netzwerkarchitektur, die wir als du- Architektur nennen, trennt explizit die Darstellung von Zustandswerten und (zustandsabhängigen) Handlungsvorteilen. Die Duelling-Architektur besteht aus zwei Strömen, die die die die Wert- und Vorteilsfunktionen repräsentieren, während sie sich ein gemeinsames Faltungsmerkmal-Lernmodul teilen. Die beiden Ströme werden über eine spezielle Aggregationsschicht kombiniert, um eine eine Schätzung der Zustands-Aktions-Wertfunktion Q, wie in Abbildung 1. Dieses Duellingnetz ist zu verstehen als ein Q-Netz mit zwei Strömen zu verstehen, das das gängige Q-Netz mit einem Strom in bestehenden Algorithmen wie Deep Q-Networks (DQN; Mnih et al., 2015). Das Duelling Netzwerk erzeugt automatisch separate Schätzungen der Zustandswertfunktion und der Vorteilsfunktion, ohne dass eine zusätzliche Überwachung.

Intuitiv kann die Duellingarchitektur lernen, welche Zustände wertvoll sind (oder nicht), ohne die Auswirkungen jeder Aktion für jeden Zustand zu lernen. Dies ist besonders nützlich in Zuständen, in denen ihre Aktionen die Umgebung nicht in relevanten Weise beeinflussen.

In den Experimenten zeigen wir, dass die Duelling-Architektur schneller die richtige Aktion während der Bewertung identifizieren kann, wenn redundante oder ähnliche Aktionen zu dem Lernproblem hinzugefügt werden.


Der Vorteil der Duell-Architektur liegt zum Teil in ihrer Fähigkeit, die Zustandswertfunktion effizient zu erlernen. Bei vielen Zuständen nicht notwendig ist, den Wert
den Wert der einzelnen Handlungsoptionen zu schätzen. Die Streams sind so konstruiert, dass
dass sie die Fähigkeit haben, getrennte Schätzungen
der Wert- und Vorteilsfunktionen zu liefern. Schließlich werden die
werden die beiden Ströme kombiniert, um eine einzige Ausgangsfunktion Q zu erzeugen.

Mit jeder Aktualisierung der Q-Werte in der Dueling-Architektur, wird der Wertestrom V aktualisiert - im Gegensatz zu den Aktualisierungen
in einer Single-Stream-Architektur, bei der nur der Wert nur der Wert für eine der Aktionen aktualisiert wird, die Werte für alle anderen
Aktionen bleiben unangetastet. Diese häufigere Aktualisierung des Wertstroms in unserem Ansatz werden mehr Ressourcen
V zu und ermöglicht so eine bessere Annäherung an die Zustandswerte
Werte, die wiederum genau sein müssen, damit auf Zeitdifferenzen
basierte Methoden wie Q-learning funktionieren (Sutton
& Barto, 1998). Dieses Phänomen spiegelt sich in den Experimenten wider,
wo der Vorteil der Duell-Architektur
gegenüber Single-Stream-Q-Netzen wächst, wenn die Anzahl der
Aktionen groß ist.
 einschließlich besserer
Wiedergabespeicher, bessere Explorationsstrategien, intrinsische Motivation,


value and advantage in tiefen Q-Netzen entkoppelt, während
ein gemeinsames Modul zum Lernen von Merkmalen. 

führt zu dramatischen Verbesserungen gegenüber bestehenden Ansätze für tiefe RL in der anspruchsvollen Atari-Domäne.

# Q-Werte entsprechen einer Metrik, die angibt, wie gut eine Aktion für einen bestimmten Zustand ist -> Aktionswertfunktion
    Die Metrik ist nichts anderes als der erwartete Ertrag dieser Aktion für den jeweiligen Zustand. Q-Werte können in der Tat in zwei Teile zerlegt werden: die Zustandswertfunktion V(s) und der Vorteilswert A(s, a):
        Q(s, a)=V(s)+A(s, a)
    Die Vorteilsfunktion gibt an, wie besser eine Aktion im Vergleich zu den anderen in einem bestimmten Zustand ist, während die Wertfunktion, wie wir wissen, angibt, wie gut es ist, sich in diesem Zustand zu befinden. Die ganze Idee hinter den Duell-Q-Netzen beruht auf der Darstellung der Q-Funktion als Summe der Wert- und der Vorteilsfunktion. Wir haben einfach zwei Netze, um jeden Teil der Summe zu lernen, und dann fassen wir ihre Ergebnisse zusammen.

Die Wertfunktion V(s) sagt uns, wie viel Belohnung wir im Zustand s erhalten werden. Und die Vorteilsfunktion A(s, a) sagt uns, wie viel besser eine Aktion im Vergleich zu den anderen Aktionen ist. Kombiniert man den Wert V und den Vorteil A für jede Aktion, erhält man die Q-Werte:


Die Agenten sind nun in der Lage, einen Zustand zu bewerten, ohne sich um die Auswirkungen der einzelnen Aktionen in diesem Zustand zu kümmern. Das bedeutet, dass die Merkmale, die bestimmen, ob ein Zustand gut oder schlecht ist, nicht notwendigerweise die gleichen sind wie die Merkmale, die eine Aktion bewerten. Und es kann sein, dass es sich überhaupt nicht um Aktionen kümmern muss. Es ist nicht ungewöhnlich, dass Aktionen in einem Zustand die Umwelt überhaupt nicht beeinflussen. Warum sollten sie also berücksichtigt werden?

* Kurzer Hinweis: Wenn Sie sich das Bild genauer ansehen, werden Sie feststellen, dass wir die Ergebnisse der beiden Netze nicht einfach addieren, um sie zusammenzufassen. Der Grund dafür ist die Frage der Identifizierbarkeit. Wenn wir das Q haben, können wir das V und das A nicht finden, also können wir nicht rückwärts propagieren. Stattdessen entscheiden wir uns dafür, den mittleren Vorteil als Basislinie zu verwenden (den subtrahierten Term). Q=20 unendlich viele Möglichkeiten für V+a

Der Duell-DQN-Algorithmus [2] schlägt vor, dass dasselbe neuronale Netz seine letzte Schicht in zwei Teile aufteilt, von denen einer die Zustandswertfunktion für den Zustand s (V(s)) und der andere die Vorteilsfunktion für jede Aktion a (A(s, a)) schätzt, und am Ende beide Teile zu einem einzigen Ausgang kombiniert, der die Q-Werte schätzt. Diese Änderung ist hilfreich, weil es manchmal nicht notwendig ist, den genauen Wert jeder Aktion zu kennen, so dass es in manchen Fällen ausreicht, nur die Zustandswertfunktion zu lernen.

