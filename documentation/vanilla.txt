Kopplung von Aktionsauswahl und Aktionsbewertung
-> Das Zielnetz berechnet Q(s, a_i) für jede mögliche Aktion a_i im Zustand s. Die gierige Strategie entscheidet sich für den höchsten Wert Q(s, a_i) und wählt die Aktion a_i aus. Das bedeutet, dass das Zielnetz die Aktion a_i auswählt und gleichzeitig deren Qualität durch die Berechnung von Q(s, a_i) bewertet. Double Q-learning versucht, diese Verfahren voneinander zu entkoppeln.

Double DQN >> Vanilla

Vanilla-Algorithm (-> .png) (https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc)
Achtung! Vanilla Q-Learning (Nicht Vanilla DQN)

1. Initialize your Q-table
2. Choose an action using the Epsilon-Greedy Exploration Strategy
3. Update the Q-table using the Bellman Equation


Vanilla Q-Learning: Eine Tabelle ordnet jedem Zustands-Aktionspaar dem entsprechenden Q-Wert zu
Deep Q-Learning: Ein neuronales Netz bildet Eingangszustände auf Paare (Aktion, Q-Wert) ab

Epsilon Greedy Exploration Strategy:
Bei jedem Zeitschritt, wenn es Zeit ist, eine Aktion zu wählen, wird gewürfelt.
Wenn der Würfel eine Wahrscheinlichkeit kleiner als Epsilon hat, wähle eine zufällige Aktion
-> Andernfalls wähle die beste bekannte Aktion im aktuellen Zustand des Agenten

