{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning mit der Cartpole Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kennenlernen der Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiieren der Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time \n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein einfacher Test einer bestehenden Umgebung aus dem gym package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of steps you run the agent for \n",
    "num_steps = 200\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # take random action, but you can also do something more intelligent\n",
    "    # action = my_intelligent_agent_fn(obs) \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # apply the action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Render the env\n",
    "    env.render()\n",
    "\n",
    "    # Wait a bit before the next frame unless you want to see a crazy fast video\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    # If the epsiode is up, then start another one\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst betrachten wir das Umgebungssetting, d.h. Aktions- und Zustandsraum, Belohnungsfunktion usw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(env.unwrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementieren des Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stein\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time \n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird der Agent instantiiert. Die Konfiguration des Agenten erfolgt über ein config-dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import DQNAgent\n",
    "\n",
    "config = {\n",
    "    \"EPISODES\": 800,\n",
    "    \"REPLAY_MEMORY_SIZE\": 1_00_000,\n",
    "    \"MINIMUM_REPLAY_MEMORY\": 1_000,\n",
    "    \"MINIBATCH_SIZE\": 64,\n",
    "    \"UPDATE_TARGETNW_STEPS\": 200,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"EPSILON\": 1,\n",
    "    \"EPSILON_DECAY\": 0.99,\n",
    "    \"MINIMUM_EPSILON\": 0.001,\n",
    "    \"DISCOUNT\": 0.99,\n",
    "    \"VISUALIZATION\": False\n",
    "}\n",
    "\n",
    "\n",
    "# Our model to solve the mountain-car problem.\n",
    "agent = DQNAgent(env, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Trainingsprozess des Agenten wird über die Methode train aufgerufen. Ein Tracken des Fortschrittes ist über tensorboard möglich. Es wird automatisch bei Start ein Ordner angelegt mit dem Datum und der Uhrzeit, in dem die Logs gespeichert werden. \n",
    "\n",
    "Achtung: Der Trainingsprozess kann länger dauern. Insbesondere durch die Problematik der Exploration in der vorliegenden Umgebung benötigt der Agent etwas Zeit um den Zustandsraum komplett zu erkunden. In dieser Zeit macht er natürlich keinen großen Lernfortschritt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haben Sie einen gewissen Lernstand erreicht können Sie ihren Agenten testen. Speichern Sie ihn mittels agent.save_model(...), oder verwenden Sie einen gespeicherten Zwischenstand, der bei einem besonderen Erfolgserlebnis (zum ersten Mal erreichte maximale Score) oder eine Anzahl an Episoden gespeichert wurde. Die derzeitige Implementierung speichert die zugehörigen .h5 files in die jeweiligen log directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model('Intermediate_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(env, '500.0_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisierung eines Agenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# instantiate the env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# load the latest weights of the DQN agent\n",
    "agent.load_model('final')   \n",
    "\n",
    "# Number of steps you run the agent for \n",
    "num_steps = 500\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # take random action, but you can also do something more intelligent\n",
    "    # action = my_intelligent_agent_fn(obs) \n",
    "    action = np.argmax(agent.model.predict(np.expand_dims(obs, axis=0)))\n",
    "    \n",
    "    # apply the action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Render the env\n",
    "    env.render()\n",
    "    \n",
    "    # If the epsiode is up, then start another one\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa3a6edb25548693c0ba22c15e5924c6c31ff08e179966a22669f57ada8c5fd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
